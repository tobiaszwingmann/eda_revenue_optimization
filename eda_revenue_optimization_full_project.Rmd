---
title: 'EDA: Revenue Optimization for an Affiliate Website'
author: "Tobias Zwingmann"
date: "9 1 2019"
output: github_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
library("rvest")
library("tidyverse")
library("xml2")
library("lubridate")
library("scales") #0.4.0
library("cluster") #2.0.4
library("rbokeh")

bind_pageview_csv_to_tbl <- function(path.names) {
  pageviews_tbl <- rbind(read_csv(path.names, 
    comment = "#", col_types = cols(.default = "c") ))
}

bind_clicks_csv_to_tbl <- function(path.names) {
 rbind(
    read_csv(path.names, skip = 1, col_types = cols(
  `Tracking-ID` = col_character(),
  Klicks = col_integer(),
  `Artikel bestellt` = col_integer(),
  `Artikel geliefert` = col_integer(),
  `Umsatz (€)` = col_double(),
  `Ad Gebühren (€)` = col_double()
  ) ) %>% mutate(File = path.names)
    )
}
  
bind_click_dates_csv_to_tbl <- function(path.names) {
  rbind(
    tibble(
    "File" = path.names,
    "Range" = read_file(path.names) %>% str_sub(start = 26, end = 49)
    )
    )
}


get_cw_and_year_from_date <- function(date){
  paste(year(date), 
                    if_else(
                      isoweek(date)<10, paste0("0",isoweek(date)),as.character(isoweek(date)) #CW 1 als CW 01 schreiben für Sortierung
                      ), sep="-")
}

hampel.proc <- function(x, t = 3, RemoveNAs = FALSE){
  #
  #  This procedure returns an index of x values declared
  #  outliers according to the Hampel detection rule, if any
  #
  mu <- median(x, na.rm = RemoveNAs)
  sig <- mad(x, na.rm = RemoveNAs)
  indx <- which( abs(x - mu) > t*sig)
  #
  indx
}
mav <- function(x,n=5){filter(x,rep(1/n,n), sides=2)}
```


#Match Website-URLs with Amazon Affiliate Tracking IDs

Idea: Scrape Website and associate URLs with Tracking ID indicated as "tag=..." in outgoing links.

##Read a Sitemap to fetch all URLs a website has for scraping
```{r}

if (file.exists("data/html_urls.rds")) {
  
  html_urls <- read_rds("data/html_urls.rds")

} else {
  
#Folgenden Code nur ausführen, wenn kein rds vorhanden ist:

 sitemap <- read_xml("https://www.welches-hdmi-kabel.de/sitemap.xml")
 # This sitemap is nested so we have to get all sitemaps first

 sitemap_urls <- sitemap %>%
   xml_children() %>%
   xml_text() %>%
   str_replace_all(".xml.*", ".xml")

 sitemap_urls <- sitemap_urls[-1]

 html_urls = tibble("value" = character())

 n = length(sitemap_urls)

 #We iterate through each sitemap and extract the URLs starting with https:...
 for (i in 1:n ){
   html_urls <- rbind(html_urls,
   sitemap_urls[i] %>%
   read_xml() %>%
   xml_children() %>%
   xml_children() %>%
   xml_text() %>%
   as_tibble() %>%
   filter(str_detect(value, 'https:'))
 )
   }
 write_rds(html_urls, "data/html_urls.rds")
}


```

##Scrape the URLs from the sitemap for Amazon Affiliate Tag. If found, write into table and link to the resp. URL. Count the number of occurences of outgoing links per Website. 
```{r}
#Scrape Webseiten nach Amazon Affiliate Tag

if (file.exists("data/matched_tags_to_url.rds")) {

  matched_tags_to_url <- read_rds("data/matched_tags_to_url.rds")

} else {

 matched_tags_to_url = tibble("Tag" = character(),
                         "n" = integer(),
                         "URL" = character())

 n = nrow(html_urls)

 j = 1L

 for (i in 1:n) {

   url <-  html_urls[[1]][i]

   parsed_url_links <-
     url %>%
     read_html() %>%
     html_nodes("body") %>%
     html_nodes("a") %>%
     html_attr("href") %>%
     as.tibble()

    if(nrow(parsed_url_links %>% filter(str_detect(value, "tag="))) > 0) {
     #Code wird nur ausgeführt, wenn ein ausgehender Link mit Tag= auf der Webseite zu finden ist:
      matched_tags_to_url[j,] <-
       parsed_url_links %>%
       filter(str_detect(value, "tag=")) %>%
       mutate(Tag = str_extract(value, "tag=.*-21")) %>%
       mutate(Tag = str_remove(Tag, "tag=")) %>%
       group_by(Tag) %>%
       count %>%
       mutate(URL = url)
      j <- j+1L
    }

 }
 rm(parsed_url_links, url, j, i, n)
 
 #Für solche Fälle, in denen eine URL einen Tag enthält, der nicht über Amazon Partnernet getrackt wird (z.B. Amazon US-Programm, entsteht ein NA. Diese hier ausschließen für weitere Analyse.)
 matched_tags_to_url <- matched_tags_to_url %>% 
   filter(!is.na(Tag))

  write_rds(matched_tags_to_url, "data/matched_tags_to_url.rds")

}

```


# Umsätze (Orders) von Amazon
```{r}
#Umsätze
fee_orders_xml <- read_xml("data/Amazon Orders 18.12.2017 - 16.12.2018/1549029224178-Fee-Orders-7a68a8a6-7342-4f73-a750-3e26a56c5147-XML.xml")

fee_orders_xml <-
  fee_orders_xml %>% 
  xml_children() %>% 
  xml_children() %>% 
  xml_attrs()

fee_orders_tbl <-
  fee_orders_xml %>%
  as.data.frame() %>% 
  t %>% 
  as_tibble(row.names=F)

rm(fee_orders_xml)

#Aggregiere Bestellungen auf Tagesebene
fee_orders_daily_tbl <-
  fee_orders_tbl %>% 
  mutate(Datum = parse_date_time(Datum, "Ymd HMS")) %>% 
  mutate(Datum = as.Date(Datum)) %>% 
  mutate(Preis = as.numeric(str_replace(Preis, ",", ".")),
         Menge = as.integer(Menge)) %>%
  mutate(Umsatz = Preis * Menge) %>% 
  group_by(Tag, Datum) %>% 
  summarise(Umsatz = sum(Umsatz)) %>% 
  mutate(CW = get_cw_and_year_from_date(Datum))

fees_weekly_tbl <- fee_orders_daily_tbl %>% 
  group_by(Tag, CW) %>%
  summarise(Umsatz = sum(Umsatz))

```

## Klicks auf Amazon Partnerlinks
```{r}
file.names <- list.files("data/Amazon Klicks 18.12.2017 - 16.12.2018/")
path.names <- paste0("data/Amazon Klicks 18.12.2017 - 16.12.2018/", file.names)

clicks_tbl <- tibble(
  "Tracking-ID" = character(),
  "Klicks" = numeric(),
  "Artikel bestellt" = numeric(),
  "Artikel geliefert" = numeric(),
  "Umsatz (€)" = numeric(),
  "Ad Gebühren (€)"  = numeric()
  )

clicks_tbl <- map_df(path.names, bind_clicks_csv_to_tbl) #Diese Tabelle enthält noch keine Angaben zum Datum

clicks_date_tbl <- map_df(path.names, bind_click_dates_csv_to_tbl) #Diese Tabelle enthält nur Datum und Dateiname

clicks_weekly_tbl <-
  clicks_tbl %>% 
  left_join(clicks_date_tbl) %>% 
  separate(Range, c("Start", "End"), " to ") %>% 
  select(-File) %>% 
  mutate(Start = mdy(Start),
         End = mdy(End),
         CW = get_cw_and_year_from_date(Start),
         CW_check = get_cw_and_year_from_date(End),
         DaysInWeek = End - Start + 1
         )%>% 
  arrange(CW) %>%
  rename(Tag = `Tracking-ID`)

# Sanity Checks: Die einzelnen CSV-Files wurden manuell aus dem Amazon Partnernet generiert.
# Hier wird geprüft, ob Start- und Enddatum des jeweiligen CSV-Files in der gleichen Kalenderwoche sind und ob
# jede Kalenderwoche aus 7 Einzeltagen besteht.

if (all(clicks_weekly_tbl$CW == clicks_weekly_tbl$CW_check) & all(clicks_weekly_tbl$DaysInWeek == 7)) {
  clicks_weekly_tbl <- 
    clicks_weekly_tbl %>% 
    group_by(Tag, CW) %>% 
    summarise(Klicks = sum(Klicks))
} else {
  print("Prüfe Rohdaten im Ordner Amazon Klicks")
}
```


# Pageviews from Google Analytics
```{r message=FALSE, warning=FALSE}

file.names <- list.files("data/Google Analytics 01.12.2017 - 31.12.2018/")
path.names <- paste0("data/Google Analytics 01.12.2017 - 31.12.2018/", file.names)

pageviews_tbl <- tibble(
  "Page path level 1" = character(),
  "Date" = character(),
  "Pageviews" = character(),
  "Unique Pageviews" = character(),
  "Avg. Time on Page" = character(),
  "Bounce Rate" = character(),
  "% Exit" = character()
  )


pageviews_tbl <- map_df(path.names, bind_pageview_csv_to_tbl)
#Parsing Fehler verursacht durch CSV-Format, welches eigentlich 2 Berichte in einer Datei enthält

#Clean up data frame 
pageviews_tbl <- pageviews_tbl %>% 
  filter(!is.na(`Page path level 1`)) %>% 
  filter(!is.na(Date)) %>% 
  mutate(URL = str_remove(`Page path level 1`, "/$")) %>% #Entferne Slashes am Ende
  filter(str_detect(URL, "^/")) %>% 
  mutate(Pageviews = str_remove(Pageviews, ","),
         Pageviews = as.numeric(Pageviews)) %>% 
  group_by(URL, Date) %>%
  summarise(Pageviews = sum(Pageviews)) %>%
  ungroup() %>% 
  mutate(URL = paste0("https://www.welches-hdmi-kabel.de",URL)) %>% 
  mutate(CW = get_cw_and_year_from_date(ymd(Date)))

pageviews_weekly_tbl <- 
  pageviews_tbl %>% 
  group_by(URL, CW) %>% 
  summarize(Pageviews = sum(Pageviews))

```

##Zusammenführen von Views, Klicks und Umsätzen in ein einheitliches Datenset
```{r}
# Kleinster Granularitätsfaktor: Kalenderwoche Tag und URL
date_range <- fees_weekly_tbl$CW %>% unique() %>% sort

weekly_raw_tbl <-
  expand(matched_tags_to_url, Tag, date_range ) %>%  #Basis Dataframe mit jeder Kombination aus Tag und CW in Daterange
  rename(CW = date_range) %>% 
  left_join(matched_tags_to_url) %>%
  mutate(URL = str_remove(URL, "/$")) %>% 
  left_join(pageviews_weekly_tbl, by=c("URL", "CW")) %>% 
  left_join(clicks_weekly_tbl, by=c("CW", "Tag")) %>% 
  left_join(fees_weekly_tbl, by=c("CW", "Tag")) %>%
  mutate(Pageviews = replace_na(Pageviews, 0),
         Klicks = replace_na(Klicks, 0),
         Umsatz = replace_na(Umsatz, 0)) %>% 
  mutate(Umsatz = replace_na(Umsatz, 0)) %>% 
  group_by(Tag, CW) %>% 
  summarise(Pageviews = sum(Pageviews),
            Klicks = sum(Klicks),
            Umsatz = sum(Umsatz)) %>%
  ungroup() %>% 
  mutate(CTR = ifelse(Pageviews == 0, 0, round(Klicks / Pageviews,4))) %>% 
  mutate(RPC = ifelse(Klicks == 0, 0, round(Umsatz / Klicks, 4)))


write_rds(weekly_raw_tbl, "data/weekly_raw_tbl.rds")

weekly_raw_tbl %>% 
  head()

```



# Exploration

## Wöchentliche Trends allgemein
### ... nach Seitenaufrufen
```{r}
#Data Transformation
weekly_raw_tbl %>% 
  select(CW, Pageviews) %>% 
  group_by(CW) %>% 
  summarise(Pageviews = sum(Pageviews)) %>%
  
  #Visualisation
  ggplot(aes(CW, Pageviews)) +
  geom_line(group=1) +
  ggtitle("Entwicklung der wöchentlichen Seitenaufrufe im Zeitverlauf") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  expand_limits(y=0)
  
```

### ... nach Klicks
```{r}
#Data Transformation
weekly_raw_tbl %>% 
  select(CW, Klicks) %>% 
  group_by(CW) %>% 
  summarise(Klicks = sum(Klicks)) %>%
  
  #Visualisation
  ggplot(aes(CW, Klicks)) +
  geom_line(group=1) +
  ggtitle("Entwicklung der wöchentlichen Klicks im Zeitverlauf") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  expand_limits(y=0)
  
```

### ... nach Umsätzen
```{r}
#Data Transformation
weekly_raw_tbl %>% 
  select(CW, Umsatz) %>% 
  group_by(CW) %>% 
  summarise(Umsatz = sum(Umsatz)) %>%
  
  #Visualisation
  ggplot(aes(CW, Umsatz)) +
  geom_line(group=1) +
  ggtitle("Entwicklung der wöchentlichen Umsätze im Zeitverlauf") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  expand_limits(y=0)
  
```

## Seitenaufrufe, Umsätze, Klicks in einem Chart
Erkenntnis: Alle dre Variablen korrelieren miteinander, der Umsatz bricht aber nicht so stark ein, wie die anderen Werte (Klicks)
```{r}
weekly_raw_tbl %>% 
  select(CW, Pageviews, Klicks, Umsatz) %>% 
  group_by(CW) %>% 
  summarise(Umsatz = sum(Umsatz),
            Klicks = sum(Klicks),
            Pageviews = sum(Pageviews)) %>%
  ungroup() %>%
  #Skaliere alle Werte zwischen 0 und 1
  mutate(Umsatz = rescale(Umsatz,to=c(0,1)),
         Klicks = rescale(Klicks, to=c(0, 1)),
         Pageviews = rescale(Pageviews, to=c(0, 1))) %>%
  gather(key = Kriterium, value = Wert, -CW) %>% 
  
  #Visualisation
  ggplot(aes(x=CW, y=Wert, color=Kriterium)) +
  geom_line(aes(group = Kriterium), size = 1.1) +
  ggtitle("Entwicklung der wöchentlichen Seitenaufrufe, Klicks und Umsätze im Zeitverlauf") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position = "bottom") +
  expand_limits(y=0)
```


#Analyse der Umsätze und Seitenaufrufe für Wochentage
```{r}
#Temporary Tables for Join
a = pageviews_tbl %>% 
  mutate(Wochentag = weekdays(ymd(Date))) %>% 
  select(Wochentag, Pageviews) %>% 
  group_by(Wochentag) %>% 
  summarise(Pageviews = sum(Pageviews)) %>% 
  mutate(Pageviews_perc = prop.table(Pageviews))


b = fee_orders_daily_tbl %>%
  ungroup() %>% 
  mutate(Wochentag = weekdays(ymd(Datum))) %>% 
  select(Wochentag, Umsatz) %>% 
  group_by(Wochentag) %>% 
  summarise(Umsatz = sum(Umsatz)) %>% 
  mutate(Umsatz_perc = prop.table(Umsatz))

#Data Transformation
left_join(a, b, by="Wochentag") %>% 
  select(Wochentag, Pageviews_perc, Umsatz_perc) %>% 
  gather(key = Kriterium, value = Prozent, -Wochentag) %>%
  mutate(Wochentag = fct_relevel(Wochentag,c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>% 
  
  ggplot(aes(x=Wochentag, y=Prozent, fill=Kriterium)) +
  geom_bar(position="dodge", stat="identity")
  
```


## Analyse der Kennzahlen CTR und RPC

Wir sind interessiert an einem Benchmark der Kennzahlen CTR und RPC. Die erste Frage: Gibt es innerhalb dieser Kennzahlen überhaupt Varianz und wenn ja wie stark ist diese ausgeprägt?

### Verteilung der CTR insgesamt
Einschränkung: Seitenaufrufe mindestens 50 (hier: pro Woche). Sonst keine verlässlichen Zahlen möglich.
Es scheint mindestens 3 Peaks zu geben: um 0.18, 0.27 und unter 0.05.
Erkenntnis: Ja, es existiert Varianz. Die CTR streut sogar relativ stark um den Mittelwert XXX. Wobei Mittelwert hier offensichtlich kein besonders gutes Lagemaß ist.
```{r}
CTR_mean <- weekly_raw_tbl %>% filter(Pageviews >= 50) %>% select(CTR) %>% unlist %>% mean

weekly_raw_tbl %>%
  filter(Pageviews >= 50) %>% 
  select(CTR) %>%
  ggplot(aes(x=CTR)) +
  geom_histogram(binwidth = 0.01) + 
  geom_vline(aes(xintercept=CTR_mean))
```

Schwankt der CTR in Abhängigkeit von der Zeit?
Ergebnis: Nein, der CTR ist über die Zeit relativ konstant, hier scheint kein Muster vorzuliegen
```{r}
weekly_raw_tbl %>%
  filter(Pageviews >= 50) %>% 
  select(CTR, CW) %>%
  ggplot(aes(y=CTR, x=CW)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Vermutung: CTR variiert je nach Seiten-URL, also manche Seiten performen gut, andere weniger.
Erkenntnis: Stimmt, der CTR schwankt stark nach Seiten-URL (Mittelwert 0.4 und 0.01). Die Sortierung der X-Achse hier erfolgt anhand der absteigenden Seitenaufrufe gesamt dieser Seiten-URL. Zwei Top-URLs haben katastrophale CTR-Werte
```{r}
tag_id_order <- weekly_raw_tbl %>%
  filter(Pageviews >= 50) %>%
  select(Tag, Pageviews) %>% 
  group_by(Tag) %>% 
  summarize(Pageviews = sum(Pageviews)) %>%
  arrange(desc(Pageviews)) %>% 
  select(Tag) %>% 
  unlist

weekly_raw_tbl %>%
  arrange(desc(Pageviews)) %>% 
  filter(Pageviews >= 50) %>% 
  select(CTR, Tag) %>%
  mutate(Tag = fct_relevel(Tag, tag_id_order)) %>% 
  ggplot(aes(y=CTR, x=Tag)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Wie verhält sich die CTR im Vergleich zu Seitenaufrufen? Schwankt die CTR in Abhängigkeit von den absoluten Seitenaufrufen?
Zur Verdeutlichung schauen wir uns den direkten Zusammenhang zwischen Klicks und Pageviews an. Sollte die CTR konstant sein, müssten die Punkte durch eine Regressionsgerade verbunden werden können.

Erkenntnis: Es existiert tatsächlich ein linearer Zusammenhang, je mehr Pageviews, desto mehr Klicks. Allerdings scheinen hier mindestens 2 Gruppen von Daten zu existieren, einige mit guter und einige mit schlechter Performance.

Die Modellierung des CTR-Benchmarks sollte mindestens in Abhängigkeit der Pageviews durchgeführt werden.
```{r}
weekly_raw_tbl %>%
  filter(Pageviews >= 50) %>% 
  select(Klicks, Pageviews) %>%
  ggplot(aes(y=Klicks, x=Pageviews)) +
  geom_point()
```

Modellierungsversuch der o.g. Verteilung:

```{r}
clicks_lm <- lm(Klicks ~ Pageviews, data = weekly_raw_tbl)
predicted_clicks <- data.frame(Klicks_Prediction = predict(clicks_lm, weekly_raw_tbl), Pageviews=weekly_raw_tbl$Pageviews)

weekly_raw_tbl %>%
  filter(Pageviews >= 1) %>% 
  select(Klicks, Pageviews) %>%
  ggplot(aes(y=Klicks, x=Pageviews)) +
  geom_point() +
  geom_line(data=predicted_clicks, aes(x=Pageviews, y=Klicks_Prediction), color="blue")

```

Qualität des Baseline-Modells:
Schwache Resultate. Vor allem: Verlertzung der Annahme der Normalverteilung.
```{r}
summary(clicks_lm) #r2 0.594
plot(clicks_lm, which=1)

```

Cluster-Analyse: Auflösung der zwei Sub-Gruppen

```{r}
ctr_df <- weekly_raw_tbl %>%
  filter(Pageviews > 1) %>% 
  select(Klicks, Pageviews, CTR, Tag) %>% 
  mutate(Klicks_scaled = scale(Klicks),
         Pageviews_scaled = scale(Pageviews))

ctr_cluster_list <- kmeans(ctr_df %>% select(Pageviews_scaled, Klicks_scaled), 10, nstart=50)

ctr_df$Cluster <- ctr_cluster_list$cluster

ctr_df <- ctr_df %>% 
  mutate(Cluster = as.factor(Cluster)) 

ctr_df %>% 
  ggplot(aes(x=Pageviews, y=Klicks, color=Cluster)) +
  geom_point()
```

CTR-Analyse der Cluster
```{r}
median_ctr <- median(ctr_df$CTR)

ctr_df %>% 
  ggplot(aes(x=Cluster, y=CTR)) +
  geom_boxplot() +
  geom_hline(yintercept = median_ctr) +
  coord_cartesian(ylim = c(0,1))

#Cluster aussortieren, die deutlich vom CTR-Median entfernt liegen.
clusters_perform <- ctr_df %>% 
  group_by(Cluster) %>% 
  summarise(CTR = median(CTR)) %>% 
  filter(CTR > median_ctr*0.5) %>% 
  select(Cluster) %>% 
  unlist

```

```{r}
ctr_perform_df <- ctr_df %>% 
  filter(Cluster %in% clusters_perform) %>% 
  filter(Klicks >= 10)

ctr_perform_df %>% 
  ggplot(aes(x=Pageviews, y=Klicks, color=Cluster)) +
  geom_point()
```

Modellierung: Regression nach Clustering
Zuerst: Normalverteilung erreichen
```{r echo=FALSE}
#Log-Transformation der Predictors zur besseren Modellierung:
ctr_perform_df <- ctr_perform_df %>% 
  mutate(logKlicks = log1p(Klicks),
         logViews = log1p(Pageviews))

ctr_perform_df %>% 
  ggplot(aes(y=logViews, x=logKlicks)) +
  geom_point()


```

Neue Regressionsmodellierung
```{r}
#Temporäres Modell zur Ausreißererkennung in den Reisduen
clicks_lm_1 <- lm(logKlicks ~ logViews, data = ctr_perform_df)
#Ausreißer markieren
ctr_perform_outliers <- hampel.proc(clicks_lm_1$residuals)

#Neues Modell ohne Ausreißer
clicks_lm_2 <- lm(logKlicks ~ logViews, data=ctr_perform_df[-ctr_perform_outliers,])
print(summary(clicks_lm_2)) #.83 r-squared
plot(clicks_lm_2, which=1:4)
print(shapiro.test(clicks_lm_2$residuals)) #.9755

#Normalverteilung der Residuen
clicks_lm_2$residuals %>% 
  as_tibble() %>% 
  rename(Residuals = value) %>% 
  ggplot(aes(x=Residuals)) +
  geom_histogram(binwidth = .5) +
  theme_minimal() +
  ggtitle("Residuals of Model Clicks_LM_2")

```

ausgewähltes Modell:

```{r}
clicks_lm_2
```

Benchmarking: Berechne Benchmarks und füge diese an Gesamtdatensatz an
```{r}
weekly_raw_tbl$Klicks_Benchmark <- round(expm1(predict(clicks_lm_2, weekly_raw_tbl %>% mutate(logViews = log1p(Pageviews)))),0)

weekly_raw_tbl <- 
  weekly_raw_tbl %>% 
  mutate(Klicks_Benchmark_erreicht = ifelse(Klicks >= Klicks_Benchmark, TRUE, FALSE))


weekly_raw_tbl %>%
  filter(Pageviews >= 1) %>% 

  ggplot(aes(y=Klicks, x=Pageviews)) +
  geom_point(aes(color=Klicks_Benchmark_erreicht)) +
  geom_line(aes(x=Pageviews, y=Klicks_Benchmark), color="red")


```


### Verteilung der RPC insgesamt
Existiert bei den RPC überhaupt Varianz?
Es scheint mindestens 3 Peaks zu geben: um 0.18, 0.27 und unter 0.05.
Erkenntnis: Ja, es existiert Varianz. Die CTR streut sogar relativ stark um den Mittelwert XXX. Wobei Mittelwert hier offensichtlich kein besonders gutes Lagemaß ist.
```{r}
RPC_mean <- weekly_raw_tbl %>% select(RPC) %>% unlist %>% mean

weekly_raw_tbl %>%
  select(RPC) %>%
  ggplot(aes(x=RPC)) +
  geom_histogram(binwidth = 0.5) + 
  geom_vline(aes(xintercept=RPC_mean)) +
  coord_cartesian(xlim = c(0,10))
```

Schwankt der RPC in Abhängigkeit von der Zeit?
Ergebnis: Schwer zusagen auf Grundlage der vielen Ausreißer nach oben.
```{r}
weekly_raw_tbl %>%
  select(RPC, CW) %>%
  ggplot(aes(y=RPC, x=CW)) +
  geom_boxplot() +
  coord_cartesian(ylim = c(0,20)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_hline(yintercept = RPC_mean)
```

Vermutung: RPC variiert je nach Seiten-URL, also manche Seiten performen gut, andere weniger.
Erkenntnis: Stimmt, der RPC schwankt stark nach Seiten-URL. Die Sortierung der X-Achse hier erfolgt anhand der absteigenden Klicks gesamt dieser Seiten-URL. Eine der Top-URLs hat katastrophale CTR-Werte
```{r}
tag_id_order <- weekly_raw_tbl %>%
  select(Tag, Klicks) %>% 
  group_by(Tag) %>% 
  summarize(Klicks = sum(Klicks)) %>%
  arrange(desc(Klicks)) %>% 
  select(Tag) %>% 
  unlist

weekly_raw_tbl %>%
  select(RPC, Tag) %>%
  mutate(Tag = fct_relevel(Tag, tag_id_order)) %>% 
  ggplot(aes(y=RPC, x=Tag)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  coord_cartesian(ylim = c(0,10))
```

Wie verhält sich die RPC im Vergleich zu den Klicks?
Zur Verdeutlichung schauen wir uns den direkten Zusammenhang zwischen Umsatz und Klicks an. Sollte die RPC konstant sein, müssten sich die Punkte durch eine Regressionsgerade einfach verbinden lassen.

```{r}
weekly_raw_tbl %>%
  select(Umsatz, Klicks) %>%
  ggplot(aes(y=Umsatz, x=Klicks)) +
  geom_point()
```
Erkenntnis: Linearer Zusammenhang existiert.

Die Modellierung des CTR-Benchmarks sollte mindestens in Abhängigkeit der Pageviews durchgeführt werden.

Modellierungsversuch der o.g. Verteilung:

```{r}
revenues_lm <- lm(Umsatz ~ Klicks, data = weekly_raw_tbl)
predicted_revenues <- data.frame(Umsatz_Prediction = predict(revenues_lm, weekly_raw_tbl), Klicks=weekly_raw_tbl$Klicks)

weekly_raw_tbl %>%
  filter(Klicks >= 1) %>% 
  select(Klicks, Umsatz) %>%
  ggplot(aes(y=Umsatz, x=Klicks)) +
  geom_point() +
  geom_line(data=predicted_revenues, aes(x=Klicks, y=Umsatz_Prediction), color="blue")

```

Qualität des Baseline-Modells:
Schwache Resultate. Vor allem: Verlertzung der Annahme der Normalverteilung.
```{r}
summary(revenues_lm) #r2 0.7116
plot(revenues_lm, which=1)

```

Cluster-Analyse: Auflösung Eliminiernug bestimmter Punkte zur besseren Modellierung

```{r}
rpc_hclust <- weekly_raw_tbl %>% 
  select(Tag, RPC, Klicks, Umsatz, CW) %>% 
  mutate(RPC = scale(RPC, center = TRUE, scale = TRUE) ) %>% 
  select(-Klicks, -Umsatz) %>% 
  spread(Tag, RPC) %>% 
  select(-CW)

min(rpc_hclust, na.rm = T)

#Replace missing with minimal value
rpc_hclust <- 
  rpc_hclust %>% 
  mutate_all(funs(replace(., is.na(.), min(rpc_hclust, na.rm = T)))) %>% 
  t()

rpc_hclust <- hclust(dist(rpc_hclust, method = "euclidean"), method="ward.D")
plot(rpc_hclust)

rpc_hclust_assignment <- cutree(rpc_hclust, h=20) %>% 
  as.list() %>% 
  as_tibble() %>% 
  t()

rpc_hclust_assignment = data.frame("Tag" = rownames(rpc_hclust_assignment),
                                   "Cluster" = rpc_hclust_assignment[,1])

rpc_df <- weekly_raw_tbl %>% 
  left_join(rpc_hclust_assignment, by="Tag") %>% 
  mutate(Cluster = as.factor(Cluster)) 

#Clicks vs. Views in each cluster
 rpc_df %>%
  ggplot(aes(Klicks, Umsatz)) + 
  geom_point() + 
  scale_y_continuous(labels = comma) +
  ggtitle("R2: Umsatz gegen Clicks mit Cluster") +
  ylab("Umsatz") +
  xlab("Klicks") +
  facet_wrap(~Cluster, ncol=3, scales = "free")

rpc_df %>% 
  ggplot(aes(x=Klicks, y=Umsatz, color=Cluster)) +
  geom_point()
```

RPC-Analyse der Cluster
```{r}
median_rpc <- median(rpc_df$RPC)

rpc_df %>% 
  ggplot(aes(x=Cluster, y=RPC)) +
  geom_boxplot() +
  coord_cartesian(ylim = c(0,20)) +
  geom_hline(yintercept = median_rpc)

#Cluster mit hohen RPC auswählen.
clusters_perform <- rpc_df %>% 
  group_by(Cluster) %>% 
  summarise(RPC = median(RPC)) %>% 
  filter(RPC >= median_rpc) %>% 
  select(Cluster) %>% 
  unlist

```

```{r}
#Performance Datensatz: Nur Cluster mit hohen RPC und Datenpunkte mit mehr als 0 Umsatz
rpc_perform_df <- rpc_df %>% 
  filter(Cluster %in% clusters_perform) %>% 
  filter(Klicks > 0 & Umsatz > 0)

rpc_perform_df %>% 
  ggplot(aes(x=Klicks, y=Umsatz, color=Cluster)) +
  geom_point()
```

Modellierung: Regression nach Clustering
Zuerst: Normalverteilung erreichen
```{r}
#Log-Transformation der Predictors zur besseren Modellierung:
rpc_perform_df <- rpc_perform_df %>%
  mutate(logKlicks = log1p(Klicks),
         logUmsatz = log1p(Umsatz))

hist(rpc_perform_df$logKlicks)
hist(rpc_perform_df$logUmsatz)

rpc_perform_df %>% 
  ggplot(aes(y=logKlicks, x=logUmsatz)) +
  geom_point()


```

Neue Regressionsmodellierung
```{r}
#Temporäres Modell zur Ausreißererkennung in den Reisduen
rev_lm_1 <- lm(logUmsatz ~ logKlicks, data = rpc_perform_df)
#Ausreißer markieren
rev_perform_outliers <- hampel.proc(rev_lm_1$residuals)

#Neues Modell ohne Ausreißer
rev_lm_2 <- lm(logUmsatz ~ logKlicks, data=rpc_perform_df[-rev_perform_outliers,])
print(summary(rev_lm_2)) #.75 r-squared
plot(rev_lm_2, which=1:4)
print(shapiro.test(rev_lm_2$residuals)) #.99

#Normalverteilung der Residuen
rev_lm_2$residuals %>% 
  as_tibble() %>% 
  rename(Residuals = value) %>% 
  ggplot(aes(x=Residuals)) +
  geom_histogram(binwidth = .5) +
  theme_minimal() +
  ggtitle("Residuals of Model Clicks_LM_2")

```

ausgewähltes Modell:

```{r}
rev_lm_2
```

Benchmarking: Berechne Benchmarks und füge diese an Gesamtdatensatz an
```{r}
weekly_raw_tbl$Umsatz_Benchmark <- round(expm1(predict(rev_lm_2, weekly_raw_tbl %>% mutate(logKlicks = log1p(Klicks)))),0)

weekly_raw_tbl <- 
  weekly_raw_tbl %>% 
  mutate(Umsatz_Benchmark_erreicht = ifelse(Umsatz >= Umsatz_Benchmark, TRUE, FALSE))


weekly_raw_tbl %>%
  ggplot(aes(y=Umsatz, x=Klicks)) +
  geom_point(aes(color=Umsatz_Benchmark_erreicht)) +
  geom_line(aes(x=Klicks, y=Umsatz_Benchmark), color="red")


```

## Scoring Model
Predict Clicks based on regression model & calculate Benchmark cTRs for each data point

```{r}
#See the new variance of CTR and RPC Benchmarks. Far more accurate than "just" mean value
weekly_raw_tbl <-
  weekly_raw_tbl %>% 
  mutate(Benchmark_CTR = Klicks_Benchmark / Pageviews,
         Benchmark_RPC = Umsatz_Benchmark / Klicks)

weekly_raw_tbl %>% 
  select(Benchmark_CTR) %>% 
  filter(Benchmark_CTR > 0) %>% 
  summary()

weekly_raw_tbl %>% 
  select(Benchmark_RPC) %>% 
  filter(Benchmark_RPC < Inf) %>% 
  summary()

```


```{r}
scores_df <- weekly_raw_tbl %>%
  filter(Pageviews > 0) %>% #Scoring nur bei Seiten mit Seitenaufrufen durchführen
  group_by(Tag) %>% 
  summarize(Score_CTR = mean(Klicks_Benchmark_erreicht),
            Score_RPC = mean(Umsatz_Benchmark_erreicht),
            Pageviews = sum(Pageviews)) %>% 
  mutate(Handlungsfeld = ifelse(Score_CTR > 0.5 & Score_RPC <=0.5,"B",
                                ifelse(Score_CTR > 0.5 & Score_RPC > 0.5,"C",
                                       ifelse(Score_CTR <= 0.5 & Score_RPC > 0.5,"D",
                                              "A")
                                       )
                                )
         )
```

Plotte Seiten-URLs mit hoher und niedriger CTR Performance
```{r}
ctr_underperform_urls <- scores_df %>% 
  arrange(Score_CTR) %>% 
  head(5) %>% 
  select(Tag) %>% 
  unlist

ctr_overperform_urls <- scores_df %>% 
  arrange(Score_CTR) %>% 
  tail(5) %>% 
  select(Tag) %>% 
  unlist

rpc_underperform_urls <- scores_df %>% 
  arrange(Score_RPC) %>% 
  head(5) %>% 
  select(Tag) %>% 
  unlist

rpc_overperform_urls <- scores_df %>% 
  arrange(Score_RPC) %>% 
  tail(5) %>% 
  select(Tag) %>% 
  unlist

weekly_raw_tbl %>%
  select(Tag, Pageviews, Klicks) %>%
  filter(Tag %in% ctr_underperform_urls) %>% 
  ggplot(aes(y=Klicks, x=Pageviews, color=Tag)) +
  geom_point() +
  ggtitle("Tags mit schlechter CTR Performance")

weekly_raw_tbl %>%
  select(Tag, Umsatz, Klicks) %>%
  filter(Tag %in% rpc_underperform_urls) %>% 
  ggplot(aes(y=Umsatz, x=Klicks, color=Tag)) +
  geom_point() +
  ggtitle("Tags mit schlechter RPC Performance")

```

Scoring Widget
```{r echo=FALSE}
#Include as separate file

scores_df %>% 
  figure(width=750, title="CTR-RPC-Scoring Widget", ylab="RPC Score", xlab = "CTR Score") %>% 
  ly_points(Score_CTR, Score_RPC, color = Handlungsfeld, hover=Tag, size=(sqrt(Pageviews/20))) 
```

