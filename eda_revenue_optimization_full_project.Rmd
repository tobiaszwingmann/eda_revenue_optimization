---
title: 'EDA: Revenue Optimization for an Affiliate Website'
author: "Tobias Zwingmann"
date: "9 1 2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("rvest")
library("tidyverse")
library("xml2")
library("lubridate")

bind_pageview_csv_to_tbl <- function(path.names) {
  pageviews_tbl <- rbind(read_csv(path.names, 
    comment = "#", col_types = cols(.default = "c") ))
}

bind_clicks_csv_to_tbl <- function(path.names) {
 rbind(
    read_csv(path.names, skip = 1, col_types = cols(
  `Tracking-ID` = col_character(),
  Klicks = col_integer(),
  `Artikel bestellt` = col_integer(),
  `Artikel geliefert` = col_integer(),
  `Umsatz (€)` = col_double(),
  `Ad Gebühren (€)` = col_double()
  ) ) %>% mutate(File = path.names)
    )
}
  
bind_click_dates_csv_to_tbl <- function(path.names) {
  rbind(
    tibble(
    "File" = path.names,
    "Range" = read_file(path.names) %>% str_sub(start = 26, end = 49)
    )
    )
}


get_cw_and_year_from_date <- function(date){
  paste(year(date), 
                    if_else(
                      week(date)<10, paste0("0",week(date)),as.character(week(date)) #CW 1 als CW 01 schreiben für Sortierung
                      ), sep="-")
}
```

#Match Website-URLs with Amazon Affiliate Tracking IDs

Idea: Scrape Website and associate URLs with Tracking ID indicated as "tag=..." in outgoing links.

##Read a Sitemap to fetch all URLs a website has for scraping
```{r}

html_urls <- read_rds("data/html_urls.rds")

#Folgenden Code nur ausführen, wenn kein rds vorhanden ist:

# sitemap <- read_xml("https://www.welches-hdmi-kabel.de/sitemap.xml")
# # This sitemap is nested so we have to get all sitemaps first
# 
# sitemap_urls <- sitemap %>% 
#   xml_children() %>% 
#   xml_text() %>% 
#   str_replace_all(".xml.*", ".xml")
# 
# sitemap_urls <- sitemap_urls[-1]
# 
# html_urls = tibble("URL" = character()) 
#   
# n = length(sitemap_urls)
# 
# for (i in 1:n ){
#     html_urls[i,] <- 
#     sitemap_urls[i] %>% 
#     read_xml() %>% 
#     xml_children() %>% 
#     xml_child() %>% 
#     xml_text()
#   }
# 
# write_rds(html_urls, "data/html_urls.rds")


```

##Scrape the URLs from the sitemap for Amazon Affiliate Tag. If found, write into table and link to the resp. URL. Count the number of occurences of outgoing links per Website. 
```{r}
#Scrape Webseiten nach Amazon Affiliate Tag

matched_tags_to_url <- read_rds("data/matched_tags_to_url.rds")


# matched_tags_to_url = tibble("Tag" = character(),
#                         "n" = integer(),
#                         "URL" = character()) 
#   
# n = nrow(html_urls)
# 
# j = 1L
# 
# for (i in 1:n) {
# 
#   url <-  html_urls[[1]][i]
#   
#   parsed_url_links <-
#     url %>% 
#     read_html() %>% 
#     html_nodes("body") %>% 
#     html_nodes("a") %>% 
#     html_attr("href") %>% 
#     as.tibble() 
#     
#    if(nrow(parsed_url_links %>% filter(str_detect(value, "tag="))) > 0) {
#     #Code wird nur ausgeführt, wenn ein ausgehender Link mit Tag= auf der Webseite zu finden ist:
#      matched_tags_to_url[j,] <-  
#       parsed_url_links %>% 
#       filter(str_detect(value, "tag=")) %>% 
#       mutate(Tag = str_extract(value, "tag=.*-21")) %>% 
#       mutate(Tag = str_remove(Tag, "tag=")) %>% 
#       group_by(Tag) %>% 
#       count %>% 
#       mutate(URL = url)
#      j <- j+1L
#    }
#    
# }
# rm(parsed_url_links, url, j, i, n)
# 
# write_rds(matched_tags_to_url, "data/matched_tags_to_url.rds")



```


# Umsätze (Orders) von Amazon
```{r}
#Umsätze
fee_orders_xml <- read_xml("data/Amazon Orders 17.12.2017 - 15.12.2018/1545149503853-Fee-Orders-4490ef86-7b35-45eb-822c-b2d4cf00da00-XML.xml")

fee_orders_xml <-
  fee_orders_xml %>% 
  xml_children() %>% 
  xml_children() %>% 
  xml_attrs()

fee_orders_tbl <-
  fee_orders_xml %>%
  as.data.frame() %>% 
  t %>% 
  as_tibble(row.names=F)

rm(fee_orders_xml)

#Aggregiere Bestellungen auf Tagesebene
fee_orders_daily_tbl <-
  fee_orders_tbl %>% 
  mutate(Datum = parse_date_time(Datum, "Ymd HMS")) %>% 
  mutate(Datum = as.Date(Datum)) %>% 
  mutate(Preis = as.numeric(str_replace(Preis, ",", ".")),
         Menge = as.integer(Menge)) %>%
  mutate(Umsatz = Preis * Menge) %>% 
  group_by(Tag, Datum) %>% 
  summarise(Umsatz = sum(Umsatz)) %>% 
  mutate(CW = get_cw_and_year_from_date(Datum))

fees_weekly_tbl <- fee_orders_daily_tbl %>% 
  group_by(Tag, CW) %>%
  summarise(Umsatz = sum(Umsatz))

```

## Klicks auf Amazon Partnerlinks
```{r}
file.names <- list.files("data/Amazon Klicks 17.12.2017 - 15.12.2018/")
path.names <- paste0("data/Amazon Klicks 17.12.2017 - 15.12.2018/", file.names)

clicks_tbl <- tibble(
  "Tracking-ID" = character(),
  "Klicks" = numeric(),
  "Artikel bestellt" = numeric(),
  "Artikel geliefert" = numeric(),
  "Umsatz (€)" = numeric(),
  "Ad Gebühren (€)"  = numeric()
  )

clicks_tbl <- map_df(path.names, bind_clicks_csv_to_tbl) #Diese Tabelle enthält noch keine Angaben zum Datum

clicks_date_tbl <- map_df(path.names, bind_click_dates_csv_to_tbl) #Diese Tabelle enthält nur Datum und Dateiname

clicks_weekly_tbl <-
  clicks_tbl %>% 
  left_join(clicks_date_tbl) %>% 
  separate(Range, c("Start", "End"), " to ") %>% 
  select(-File) %>% 
  mutate(Start = mdy(Start),
         End = mdy(End),
         CW = get_cw_and_year_from_date(Start)
         )%>% 
  arrange(CW) %>% 
  rename(Tag = `Tracking-ID`) %>% 
  group_by(Tag, CW) %>% 
  summarise(Klicks = sum(Klicks))

#check with vauesfrom source files
```


# Pageviews from Google Analytics
```{r message=FALSE, warning=FALSE}

file.names <- list.files("data/Google Analytics 17.12.2017 - 15.12.2018/")
path.names <- paste0("data/Google Analytics 17.12.2017 - 15.12.2018/", file.names)

pageviews_tbl <- tibble(
  "Page path level 1" = character(),
  "Date" = character(),
  "Pageviews" = character(),
  "Unique Pageviews" = character(),
  "Avg. Time on Page" = character(),
  "Bounce Rate" = character(),
  "% Exit" = character()
  )


pageviews_tbl <- map_df(path.names, bind_pageview_csv_to_tbl)
#Parsing Fehler verursacht durch CSV-Format, welches eigentlich 2 Berichte in einer Datei enthält

#Clean up data frame 
pageviews_tbl <- pageviews_tbl %>% 
  filter(!is.na(`Page path level 1`)) %>% 
  mutate(URL = str_remove(`Page path level 1`, "/$")) %>% #Entferne Slashes am Ende
  filter(str_detect(URL, "^/")) %>% 
  mutate(Pageviews = str_remove(Pageviews, ","),
         Pageviews = as.numeric(Pageviews)) %>% 
  group_by(URL, Date) %>%
  summarise(Pageviews = sum(Pageviews)) %>%
  ungroup() %>% 
  mutate(URL = paste0("https://www.welches-hdmi-kabel.de",URL)) %>% 
  mutate(CW = get_cw_and_year_from_date(ymd(Date)))

pageviews_weekly_tbl <- 
  pageviews_tbl %>% 
  group_by(URL, CW) %>% 
  summarize(Pageviews = sum(Pageviews))

```

##Zusammenführen von Views, Klicks und Umsätzen in ein einheitliches Datenset
```{r}
# Kleinster Granularitätsfaktor: Kalenderwoche Tag und URL

weekly_raw_tbl <- 
  matched_tags_to_url %>%
  mutate(URL = str_remove(URL, "/$")) %>% 
  left_join(pageviews_weekly_tbl, by="URL") %>% 
  left_join(clicks_weekly_tbl, by=c("CW", "Tag")) %>% 
  left_join(fees_weekly_tbl, by=c("CW", "Tag")) %>% 
  mutate(Umsatz = replace_na(Umsatz, 0))

weekly_raw_tbl %>% 
  head()
```

























