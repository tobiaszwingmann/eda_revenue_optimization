---
title: 'EDA: Revenue Optimization for an Affiliate Website'
author: "Tobias Zwingmann"
date: "9 1 2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("rvest")
library("tidyverse")
library("xml2")
library("lubridate")
library("scales") #0.4.0
library("cluster") #2.0.4
library("rbokeh")

bind_pageview_csv_to_tbl <- function(path.names) {
  pageviews_tbl <- rbind(read_csv(path.names, 
    comment = "#", col_types = cols(.default = "c") ))
}

bind_clicks_csv_to_tbl <- function(path.names) {
 rbind(
    read_csv(path.names, skip = 1, col_types = cols(
  `Tracking-ID` = col_character(),
  Klicks = col_integer(),
  `Artikel bestellt` = col_integer(),
  `Artikel geliefert` = col_integer(),
  `Umsatz (€)` = col_double(),
  `Ad Gebühren (€)` = col_double()
  ) ) %>% mutate(File = path.names)
    )
}
  
bind_click_dates_csv_to_tbl <- function(path.names) {
  rbind(
    tibble(
    "File" = path.names,
    "Range" = read_file(path.names) %>% str_sub(start = 26, end = 49)
    )
    )
}


get_cw_and_year_from_date <- function(date){
  paste(year(date), 
                    if_else(
                      isoweek(date)<10, paste0("0",isoweek(date)),as.character(isoweek(date)) #CW 1 als CW 01 schreiben für Sortierung
                      ), sep="-")
}

hampel.proc <- function(x, t = 3, RemoveNAs = FALSE){
  #
  #  This procedure returns an index of x values declared
  #  outliers according to the Hampel detection rule, if any
  #
  mu <- median(x, na.rm = RemoveNAs)
  sig <- mad(x, na.rm = RemoveNAs)
  indx <- which( abs(x - mu) > t*sig)
  #
  indx
}
mav <- function(x,n=5){filter(x,rep(1/n,n), sides=2)}
```

#Match Website-URLs with Amazon Affiliate Tracking IDs

Idea: Scrape Website and associate URLs with Tracking ID indicated as "tag=..." in outgoing links.

##Read a Sitemap to fetch all URLs a website has for scraping
```{r}

if (file.exists("data/html_urls.rds")) {
  
  html_urls <- read_rds("data/html_urls.rds")

} else {
  
#Folgenden Code nur ausführen, wenn kein rds vorhanden ist:

 sitemap <- read_xml("https://www.welches-hdmi-kabel.de/sitemap.xml")
 # This sitemap is nested so we have to get all sitemaps first

 sitemap_urls <- sitemap %>%
   xml_children() %>%
   xml_text() %>%
   str_replace_all(".xml.*", ".xml")

 sitemap_urls <- sitemap_urls[-1]

 html_urls = tibble("value" = character())

 n = length(sitemap_urls)

 #We iterate through each sitemap and extract the URLs starting with https:...
 for (i in 1:n ){
   html_urls <- rbind(html_urls,
   sitemap_urls[i] %>%
   read_xml() %>%
   xml_children() %>%
   xml_children() %>%
   xml_text() %>%
   as_tibble() %>%
   filter(str_detect(value, 'https:'))
 )
   }
 write_rds(html_urls, "data/html_urls.rds")
}


```

##Scrape the URLs from the sitemap for Amazon Affiliate Tag. If found, write into table and link to the resp. URL. Count the number of occurences of outgoing links per Website. 
```{r}
#Scrape Webseiten nach Amazon Affiliate Tag

if (file.exists("data/matched_tags_to_url.rds")) {

  matched_tags_to_url <- read_rds("data/matched_tags_to_url.rds")

} else {

 matched_tags_to_url = tibble("Tag" = character(),
                         "n" = integer(),
                         "URL" = character())

 n = nrow(html_urls)

 j = 1L

 for (i in 1:n) {

   url <-  html_urls[[1]][i]

   parsed_url_links <-
     url %>%
     read_html() %>%
     html_nodes("body") %>%
     html_nodes("a") %>%
     html_attr("href") %>%
     as.tibble()

    if(nrow(parsed_url_links %>% filter(str_detect(value, "tag="))) > 0) {
     #Code wird nur ausgeführt, wenn ein ausgehender Link mit Tag= auf der Webseite zu finden ist:
      matched_tags_to_url[j,] <-
       parsed_url_links %>%
       filter(str_detect(value, "tag=")) %>%
       mutate(Tag = str_extract(value, "tag=.*-21")) %>%
       mutate(Tag = str_remove(Tag, "tag=")) %>%
       group_by(Tag) %>%
       count %>%
       mutate(URL = url)
      j <- j+1L
    }

 }
 rm(parsed_url_links, url, j, i, n)
 write_rds(matched_tags_to_url, "data/matched_tags_to_url.rds")

}

```


# Umsätze (Orders) von Amazon
```{r}
#Umsätze
fee_orders_xml <- read_xml("data/Amazon Orders 18.12.2017 - 16.12.2018/1549029224178-Fee-Orders-7a68a8a6-7342-4f73-a750-3e26a56c5147-XML.xml")

fee_orders_xml <-
  fee_orders_xml %>% 
  xml_children() %>% 
  xml_children() %>% 
  xml_attrs()

fee_orders_tbl <-
  fee_orders_xml %>%
  as.data.frame() %>% 
  t %>% 
  as_tibble(row.names=F)

rm(fee_orders_xml)

#Aggregiere Bestellungen auf Tagesebene
fee_orders_daily_tbl <-
  fee_orders_tbl %>% 
  mutate(Datum = parse_date_time(Datum, "Ymd HMS")) %>% 
  mutate(Datum = as.Date(Datum)) %>% 
  mutate(Preis = as.numeric(str_replace(Preis, ",", ".")),
         Menge = as.integer(Menge)) %>%
  mutate(Umsatz = Preis * Menge) %>% 
  group_by(Tag, Datum) %>% 
  summarise(Umsatz = sum(Umsatz)) %>% 
  mutate(CW = get_cw_and_year_from_date(Datum))

fees_weekly_tbl <- fee_orders_daily_tbl %>% 
  group_by(Tag, CW) %>%
  summarise(Umsatz = sum(Umsatz))

```

## Klicks auf Amazon Partnerlinks
```{r}
file.names <- list.files("data/Amazon Klicks 18.12.2017 - 16.12.2018/")
path.names <- paste0("data/Amazon Klicks 18.12.2017 - 16.12.2018/", file.names)

clicks_tbl <- tibble(
  "Tracking-ID" = character(),
  "Klicks" = numeric(),
  "Artikel bestellt" = numeric(),
  "Artikel geliefert" = numeric(),
  "Umsatz (€)" = numeric(),
  "Ad Gebühren (€)"  = numeric()
  )

clicks_tbl <- map_df(path.names, bind_clicks_csv_to_tbl) #Diese Tabelle enthält noch keine Angaben zum Datum

clicks_date_tbl <- map_df(path.names, bind_click_dates_csv_to_tbl) #Diese Tabelle enthält nur Datum und Dateiname

clicks_weekly_tbl <-
  clicks_tbl %>% 
  left_join(clicks_date_tbl) %>% 
  separate(Range, c("Start", "End"), " to ") %>% 
  select(-File) %>% 
  mutate(Start = mdy(Start),
         End = mdy(End),
         CW = get_cw_and_year_from_date(Start),
         CW_check = get_cw_and_year_from_date(End),
         DaysInWeek = End - Start + 1
         )%>% 
  arrange(CW) %>%
  rename(Tag = `Tracking-ID`)

# Sanity Checks: Die einzelnen CSV-Files wurden manuell aus dem Amazon Partnernet generiert.
# Hier wird geprüft, ob Start- und Enddatum des jeweiligen CSV-Files in der gleichen Kalenderwoche sind und ob
# jede Kalenderwoche aus 7 Einzeltagen besteht.

if (all(clicks_weekly_tbl$CW == clicks_weekly_tbl$CW_check) & all(clicks_weekly_tbl$DaysInWeek == 7)) {
  clicks_weekly_tbl <- 
    clicks_weekly_tbl %>% 
    group_by(Tag, CW) %>% 
    summarise(Klicks = sum(Klicks))
} else {
  print("Prüfe Rohdaten im Ordner Amazon Klicks")
}
```


# Pageviews from Google Analytics
```{r message=FALSE, warning=FALSE}

file.names <- list.files("data/Google Analytics 01.12.2017 - 31.12.2018/")
path.names <- paste0("data/Google Analytics 01.12.2017 - 31.12.2018/", file.names)

pageviews_tbl <- tibble(
  "Page path level 1" = character(),
  "Date" = character(),
  "Pageviews" = character(),
  "Unique Pageviews" = character(),
  "Avg. Time on Page" = character(),
  "Bounce Rate" = character(),
  "% Exit" = character()
  )


pageviews_tbl <- map_df(path.names, bind_pageview_csv_to_tbl)
#Parsing Fehler verursacht durch CSV-Format, welches eigentlich 2 Berichte in einer Datei enthält

#Clean up data frame 
pageviews_tbl <- pageviews_tbl %>% 
  filter(!is.na(`Page path level 1`)) %>% 
  filter(!is.na(Date)) %>% 
  mutate(URL = str_remove(`Page path level 1`, "/$")) %>% #Entferne Slashes am Ende
  filter(str_detect(URL, "^/")) %>% 
  mutate(Pageviews = str_remove(Pageviews, ","),
         Pageviews = as.numeric(Pageviews)) %>% 
  group_by(URL, Date) %>%
  summarise(Pageviews = sum(Pageviews)) %>%
  ungroup() %>% 
  mutate(URL = paste0("https://www.welches-hdmi-kabel.de",URL)) %>% 
  mutate(CW = get_cw_and_year_from_date(ymd(Date)))

pageviews_weekly_tbl <- 
  pageviews_tbl %>% 
  group_by(URL, CW) %>% 
  summarize(Pageviews = sum(Pageviews))

```

##Zusammenführen von Views, Klicks und Umsätzen in ein einheitliches Datenset
```{r}
# Kleinster Granularitätsfaktor: Kalenderwoche Tag und URL
date_range <- fees_weekly_tbl$CW %>% unique() %>% sort

weekly_raw_tbl <-
  expand(matched_tags_to_url, Tag, date_range ) %>%  #Basis Dataframe mit jeder Kombination aus Tag und CW in Daterange
  rename(CW = date_range) %>% 
  left_join(matched_tags_to_url) %>%
  mutate(URL = str_remove(URL, "/$")) %>% 
  left_join(pageviews_weekly_tbl, by=c("URL", "CW")) %>% 
  left_join(clicks_weekly_tbl, by=c("CW", "Tag")) %>% 
  left_join(fees_weekly_tbl, by=c("CW", "Tag")) %>%
  mutate(Pageviews = replace_na(Pageviews, 0),
         Klicks = replace_na(Klicks, 0),
         Umsatz = replace_na(Umsatz, 0)) %>% 
  mutate(Umsatz = replace_na(Umsatz, 0)) %>% 
  mutate(CTR = ifelse(Pageviews == 0, 0, round(Klicks / Pageviews,4))) %>% 
  mutate(RPC = ifelse(Klicks == 0, 0, round(Umsatz / Klicks, 4)))


write_rds(weekly_raw_tbl, "data/weekly_raw_tbl.rds")

weekly_raw_tbl %>% 
  head()

```



# Exploration

## Wöchentliche Trends allgemein
### ... nach Seitenaufrufen
```{r}
weekly_raw_tbl %>% 
  select(CW, Pageviews) %>% 
  group_by(CW) %>% 
  summarise(Pageviews = sum(Pageviews)) %>%
  figure(width=750, title="Entwicklung der wöchentlichen Seitenaufrufe im Zeitverlauf") %>% 
  ly_lines(CW, Pageviews) %>% 
  theme_axis("x", major_label_orientation = 90)
```

### ... nach Klicks
```{r}
weekly_raw_tbl %>% 
  select(CW, Klicks) %>% 
  group_by(CW) %>% 
  summarise(Klicks = sum(Klicks)) %>%
  figure(width=750, title="Entwicklung der wöchentlichen Seitenaufrufe im Zeitverlauf") %>% 
  ly_lines(CW, Pageviews) %>% 
  theme_axis("x", major_label_orientation = 90)
```



















